from tkinter import *
from tkinter import ttk
import speech_recognition as sr
import cv2
import numpy as np
import os
import random
from PIL import ImageTk, Image
from matplotlib import pyplot as plt
import time
import mediapipe as mp
import pyttsx3
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')


class MyApp:

    def __init__(self, master):
        self.master = master
        master.title("Speech to Text App")



        self.transcribed_text_label = Label(master, text="", fg="black", bg="grey",font=('Arial',20,'bold'), relief=SUNKEN, borderwidth=5)
        self.transcribed_text_label.place(x=50, y=150, width=1200, height=100)


        image = Image.open('C:/Users/Rk/Desktop/major_project/new_project/mic.jpg')
        photo = ImageTk.PhotoImage(image)
        self.recognize_button = Button(master, image=photo, command=lambda: [self.start_recognition(),hide()], pady=40,
                                       width=100, height=80)
        self.recognize_button.image = photo
        self.recognize_button.place(y=10, x=100)
        global message_label
        message_label = Label(master, text="", fg="green", font=('Arial', 16), pady=10)
        message_label.place(x=50, y=300)

        self.image_frame = Frame(master, width=500, height=200, borderwidth=10, bg="black", bd=5, relief=SUNKEN,pady=20)
        self.image_frame.place(x=50, y=350, width=1200, height=150)

        self.error_msg = Label(master, text="", fg="red", font=('Arial', 16), pady=10)
        self.error_msg.place(x=50, y=250)

        def hide():
            message_label.config(text="")


    def start_recognition(self):
        global message_label
        message_label = Label(root, text="Please say something", font=('Arial', 20, 'bold'), fg='green')
        message_label.place(x=60, y=100, width=300, height=50)
        root.update()
        self.error_msg.config(text="")


        r = sr.Recognizer()


        with sr.Microphone() as source:
            # adjust for ambient noise
            r.adjust_for_ambient_noise(source)
            print("please say something")

            audio = r.listen(source)
            root.after(500, message_label.pack_forget)


        try:
            global text
            text = r.recognize_google(audio)
            sound1 = list(text.split(" "))

            self.transcribed_text_label.configure(text=text)

            folder = "C://Users//RK//Desktop//major_project//new_project//images//"
            images = os.listdir(folder)
            newsize = (70, 70)
            conc_imag = []

            current_width = 0

            def func(image):
                nonlocal current_width
                img = cv2.imread("C://Users//RK//Desktop//major_project//new_project//images//" + image)
                resized_img = cv2.resize(img, newsize)
                if current_width + resized_img.shape[1] <= self.image_frame.winfo_width():
                    conc_imag.append(resized_img)
                    current_width += resized_img.shape[1]
                else:
                    conc_imag.append(np.zeros((70, 70, 3), np.uint8))
                    conc_imag.append(resized_img)
                    current_width = resized_img.shape[1]

            for i in sound1:
                x = i + '.jpg'
                if x in images:
                    func(x)
                    conc_imag.append(np.zeros((70, 70, 3), np.uint8))
                else:
                    alpha = [char for char in i]
                    for j in alpha:
                        y = j + '.jpg'
                        func(y)
                    conc_imag.append(np.zeros((70, 70, 3), np.uint8))

            conc_imag = np.concatenate(conc_imag, axis=1)
            conc_imag = cv2.cvtColor(conc_imag, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(conc_imag)
            img = ImageTk.PhotoImage(img)
            for widget in self.image_frame.winfo_children():
                widget.destroy()
            panel = Label(self.image_frame, image=img)
            panel.image = img
            panel.pack()

        except sr.UnknownValueError:
            global error_msg
            self.error_msg.config(text="Could not understand audio.")
        except sr.RequestError as e:
            self.error_msg.config(
                text="Could not request results from Google Speech Recognition service; {0}".format(e))
        if self.error_msg.cget("text") == "":
            self.error_msg.place_forget()
        else:
            self.error_msg.place(x=50, y=250)
def speech():
    engine = pyttsx3.init()
    engine.setProperty('rate', 150)
    engine.setProperty('voice', 'english-us')
    engine.say(text)
    engine.runAndWait()
def use_code():

    mp_holistic = mp.solutions.holistic
    mp_drawing = mp.solutions.drawing_utils

    def mediapipe_detection(image, model):
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        image.flags.writeable = False
        results = model.process(image)
        image.flags.writeable = True
        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
        return image, results

    def draw_landmarks(image, results):
        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS)
        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

    def draw_styled_landmarks(image, results):

        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,
                                  mp_drawing.DrawingSpec(color=(80, 110, 10), thickness=1, circle_radius=1),
                                  mp_drawing.DrawingSpec(color=(80, 256, 121), thickness=1, circle_radius=1)
                                  )

        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                                  mp_drawing.DrawingSpec(color=(80, 22, 10), thickness=2, circle_radius=4),
                                  mp_drawing.DrawingSpec(color=(80, 44, 121), thickness=2, circle_radius=2)
                                  )

        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                                  mp_drawing.DrawingSpec(color=(121, 22, 76), thickness=2, circle_radius=4),
                                  mp_drawing.DrawingSpec(color=(121, 44, 250), thickness=2, circle_radius=2)
                                  )

        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=4),
                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)
                                  )

    cap = cv2.VideoCapture(0)

    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        while cap.isOpened():

            ret, frame = cap.read()

            image, results = mediapipe_detection(frame, holistic)
            print(results)

            draw_styled_landmarks(image, results)

            cv2.imshow('OpenCV Feed', image)

            if cv2.waitKey(1) == ord('q'):
                break
        cap.release()
        cv2.destroyAllWindows()
    draw_landmarks(frame, results)

    pose = []
    for res in results.pose_landmarks.landmark:
        test = np.array([res.x, res.y, res.z, res.visibility])
        pose.append(test)
    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in
                     results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(132)
    face = np.array([[res.x, res.y, res.z] for res in
                     results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)
    lh = np.array([[res.x, res.y, res.z] for res in
                   results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(
        21 * 3)
    rh = np.array([[res.x, res.y, res.z] for res in
                   results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros
    face = np.array([[res.x, res.y, res.z] for res in
                     results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(1404)

    def extract_keypoints(results):
        pose = np.array([[res.x, res.y, res.z, res.visibility] for res in
                         results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33 * 4)
        face = np.array([[res.x, res.y, res.z] for res in
                         results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(
            468 * 3)
        lh = np.array([[res.x, res.y, res.z] for res in
                       results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(
            21 * 3)
        rh = np.array([[res.x, res.y, res.z] for res in
                       results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(
            21 * 3)
        return np.concatenate([pose, face, lh, rh])

    result_test = extract_keypoints(results)
    result_test
    np.save('0', result_test)

    result_test = extract_keypoints(results)
    result_test

    np.save('0', result_test)
    DATA_PATH = 'L_Data'
    folder_names = os.listdir(DATA_PATH)

    print(folder_names)

    actions = np.array(folder_names)

    no_sequences = 10

    sequence_length = 10

    start_folder = 10

    for action in actions:

        for sequence in range(0, no_sequences):
            try:
                os.makedirs(os.path.join(DATA_PATH, action, str(sequence)))
            except:
                pass

    cv2.destroyAllWindows()
    from sklearn.model_selection import train_test_split
    from tensorflow.keras.utils import to_categorical

    label_map = {label: num for num, label in enumerate(actions)}
    sequences, labels = [], []
    for action in actions:
        action_path = os.path.join(DATA_PATH, action)
        for sequence in os.listdir(action_path):
            sequence_path = os.path.join(action_path, sequence)
            window = []
            for frame_num in range(sequence_length):
                frame_path = os.path.join(sequence_path, "{}.npy".format(frame_num))
                res = np.load(frame_path)
                window.append(res)
            sequences.append(window)
            labels.append(label_map[action])
    np.array(sequences).shape

    np.array(labels).shape

    X = np.array(sequences)
    X.shape

    y = to_categorical(labels).astype(int)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05)
    y_test.shape
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense
    from tensorflow.keras.callbacks import TensorBoard
    from tensorflow.keras.layers import Input

    log_dir = os.path.join('Logs')
    tb_callback = TensorBoard(log_dir=log_dir)
    model = Sequential()
    input_shape = (30, 1662)
    inputs = Input(shape=input_shape)
    x = LSTM(64)(inputs)
    outputs = Dense(1, activation='sigmoid')(x)
    # model = Model(inputs=inputs, outputs=outputs)
    # model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(10, 1662)))
    model.add(LSTM(128, return_sequences=True, activation='relu'))
    model.add(LSTM(64, return_sequences=False, activation='relu'))
    model.add(Dense(64, activation='relu'))
    model.add(Dense(32, activation='relu'))
    model.add(Dense(actions.shape[0], activation='softmax'))
    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])



    res = model.predict(X_test)

    actions[np.argmax(res[-1])]
    from scipy import stats
    colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245)]

    def prob_viz(res, actions, input_frame, colors):
        output_frame = input_frame.copy()
        for num, prob in enumerate(res):
            cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)
            cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1,
                        (255, 255, 255), 2, cv2.LINE_AA)

        return output_frame

    from sklearn.metrics import multilabel_confusion_matrix, accuracy_score
    yhat = model.predict(X_test)
    ytrue = np.argmax(y_test, axis=1).tolist()
    yhat = np.argmax(yhat, axis=1).tolist()
    multilabel_confusion_matrix(ytrue, yhat)

    sequence = []
    sentence = []
    predictions = []
    threshold = 0.5

    cap = cv2.VideoCapture(0)
    running = True

    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:
        while cap.isOpened() and running:

            ret, frame = cap.read()

            # Make detections
            image, results = mediapipe_detection(frame, holistic)
            # print(results)

            # Draw landmarks
            draw_styled_landmarks(image, results)

            keypoints = extract_keypoints(results)
            sequence.append(keypoints)
            sequence = sequence[-30:]

            if len(sequence) == 30:
                res = model.predict(np.expand_dims(sequence, axis=0))[0]

                predictions.append(np.argmax(res))

                if np.unique(predictions[-10:])[0] == np.argmax(res):
                    if res[np.argmax(res)] > threshold:
                        if len(sentence) > 0:
                            if actions[np.argmax(res)] != sentence[-1]:
                                sentence.append(actions[np.argmax(res)])
                        else:
                            sentence.append(actions[np.argmax(res)])

                if len(sentence) > 5:
                    sentence = sentence[-5:]

                gesture_name = sentence[-1] if len(sentence) > 0 else ""
                image = cv2.putText(image, gesture_name, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

            sentence_str = ' '.join(sentence)
            image = cv2.rectangle(image, (0, 0), (640, 40), (245, 117, 16), -1)
            image = cv2.putText(image, sentence_str, (3, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2,
                                cv2.LINE_AA)

            logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

            print(sentence_str)
            engine = pyttsx3.init()
            engine.setProperty('rate', 150)
            engine.setProperty('voice', 'english-us')
            engine.say(sentence_str)
            engine.runAndWait()

            cv2.imshow('OpenCV Feed', image)

            if cv2.waitKey(10) & 0xFF == ord('q'):
                break
                running = False
        cap.release()
        cv2.destroyAllWindows()
def myfunc():
    print("this is my menu")
def add_sign_page():
    root.destroy()
    exec(open("tk5.py").read(), globals())
def home_page():
    root.destroy()
    exec(open("tk3.py").read(), globals())
def about_page():
    root.destroy()
    exec(open("tk6.py").read(), globals())
root = Tk()
root.geometry("1200x900")
bg = Image.open('C:/Users/RK/Desktop/major_project/Rk/bg5.jpg')

bg = bg.resize((root.winfo_screenwidth(), root.winfo_screenheight()))

baground = Label(root, image=ImageTk.PhotoImage(bg))
baground.place(x=0, y=0, relwidth=1, relheight=1)
mymenu = Menu(root)
m1 = Menu(mymenu)
m1.add_command(label="ADD_SIGN",command=add_sign_page)
m1.add_command(label="HOME PAGE",command=home_page)
m1.add_command(label="ABOUT",command=about_page)
root.config(menu=mymenu)
mymenu.add_cascade(label="Other Functions",font=40,menu=m1)
Label(root,text="Speech to sign convertion",font=('Arial',30,'bold'),fg="red").place(y=50,x=400)
app = MyApp(root)
speech_button = Button(root,text="speech",fg='white',bg="blue",font=('Arial',15,'bold'),command=speech)
speech_button.place(x=1000, y=270)
Label(root,text="Sign to Speech convertion",font=('Arial',30,"bold"),fg='orange').place(x=400,y=520)
b3 = Button(root,text='convert gesture to text',bg='red',font=('Arial',12,"bold"),command=use_code)
b3.place(x= 50,y=600)
root.mainloop()
